
# 1장 데이터 분석 개요

### 모델링 성능 평가 기준
데이터마이닝 -> 정확도, 정밀도, 디텍트레이트, lift
시뮬레이션 -> throughput, average wating time, average queue length, time in system

### 공간분석(GIS)

### 탐색적 데이터 분석(EDA)
* 데이터를 시각화하면 이상점(outlier) 식별이 잘 된다
* 알고리즘의 학습 성능에는 전적으로 데이터의 품질과 데이터에 담긴 정보량에 달려있다
* 4가지 주제
	* 잔차계산 : 주경향에서 얼마나 벗어났는지
	* 저항성의 강조 : 부분 변동에 크게 영향을 받지 않도록
	* 자료변수의 재표현
	* 그래프를 통한 션시성


#### 표본추출 방법
* 단순랜덤추출법
* 계통추출법
* 층화추출법

# 2장 R프로그래밍 기초

* 벡터에는 한가지 자료형만 넣을 수 있음
	* 만약 다른 숫자 + 문자가 들어가면 숫자도 문자형이 된다
* c(2, 4, 6, 8) + C(1, 3, 5, 7, 9)
	* 경고 메시지와 함께 결과 출력 (3 7 11 15 11)
* merge하면 inner join 된다
```R
z = c(1:3, NA)
z==NA
# NA NA NA NA
```
* 숫자형 행렬에서 원소 중 하나를 문자형으로 변경 ==> 해당 행렬의 모든 원소가 문자형으로 바뀜
* 행렬을 as.vector함수에 입력하면 1열부터 차례로 원소를 나열하는 벡터 형성
```R
x = matrix(c(1,2,3,4), nrow=2)
# 1 3
# 2 4
as.vector(x)
# 1 2 3 4
```
```R
“+”(2,3) # 5
```
```R
y = c(1,2,3,NA)
# 3,6,9,NA
```R
x <- c(1:5)
y <- seq(10,50,10)
xy <- rbind(x,y)
# 1 2 3 4 5
# 10 20 30 40 50
```
```R
dim(m1) <- c(4,5) # 4x5행렬
apply(m1, 1, sum)# 행의 합
apply(m1, 2, sum)# 열의 합
lapply(m1,sum)# 각 셀의 합, 총20개의 해
```
```r
x<-1:100
sum(x>50) # 50
```
```r
subset(test, subset=(학과==경영학과))
sqldf()
```

# 3장 데이터마트
* reshape 패키지
	* melt, cast
	* 변수를 조합하 변수를 만들고, 결합해 요약변수와 파생변수를 쉽게 생성
```r
cast(md, id+variable~time)
sqm <- melt(airquality,id=c(“Month”,”Day”), na.rm=TRUE)
```
* 요약변수
	* 데이터마트에서 기존적인 변수
	* 재활용성이 높음
	* 다양한 모델을 개발해야 하는 경우, 효율적으로 사용
* 파생변수
	* 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 부여
	* 주간적, 논리적 타당성을 맞춰야함
* 결측치
	* 칸이 비어있는 경우 결측치 여부는 알기 쉬움
	* 관측치가 있지만 실상은 default값이 기록된 경우에도 의미가 있음
	* 결측치가 20%이상인 경우에는 해당 변수를 제거하고 분석
	* 다중대치법 : m번 대치를 통해 m개의 가상적 완전 자료를 만들어 분석, 대치 -> 분석 => 결합
	* complete.cases() : 결측치가 없으면 T, 있으면 F
	* is.na() : 결측값이 NA인지 확인
	* centrallmputation() : NA값에 가운데 값으로 대치, 숫자는 최빈값으로
	* knnImputation() : NA값을 knn을 사용해 주변 이웃의 거리를 고려야혀 대치
	* rfImpute() : 랜덤 포레스트 모양의 경우, 결측값이 있으면 에러를 발생하기 때문에 랜덤포레스트 패키지에서 NA결측값을 대치하도록 하는 함수

* 이상치
	* IQR=Q3-Q1, Q1-1.5*IQR<x<Q3+1.5*IQR
	* 평균으로 부터 3*표준편자 넘어간 값
	* 상자그림으로 확인
	* 이상치 제거여부는 실무자를 통해서 결정
	* 군집분석으로 다른 데이터들과 거리상 멀리 떨어진 데이터를 이상치로 봄
	* 데이터 측정과저이나 입력과정에서 잘못된 이상치도 의미있을 수 있어 바로 삭제하지 않음
	* 설명변수의 관측치에 비해 종속변수의 값이 상이한 값도 이상치
	* 환경파괴에는 이상치를 적용하기 어려움

* plyr패키지
	* multi-core를 사용하여 반복문을 사용하지 않고도 매우 간단하고 빠르게 데이터 처리
```r
# feed별로 weight평균값 구하기
ddply(chickwts, ~feed, summarize, groupmean=mean(weight)) 
```
# 4장 통계분석
### 표본추출 방법
* 단순랜덤 추출법
* 계통추출법 : 구간을 나누고 구간별로 K번째 자료를 추출 
* 집락추출법 : 군집을 구분하고 군집별로 단순랜덤 추출
* 층화추출법 : **이질적인 원소들로 구성된 모집단에서**각 계층을 고루 대표할 수 있는 표본 추출

#### 척도종류
* 범주형자료
	* 명목척도 : 카테고리, 집단(성별, 시/도)
	* 순서척도 : 서열관계 ( 만족도, 학년, 신용등급)
* 수치형자료
	* 구간척도(등간척도) : 속성의 양을 측정, 간격이 의미가 있음(온도, 지수) , 더하기 빼기는 가능하나 곱하기 나누기 불가
	* 비율척도 : 차이에 대한 비율이 의미를 가짐, 절대적 기준인 0이 존재, 칙연산 가능(무게, 나이, 거리)




##
* A,B가 독립이면 각 조건부확률은 독립확률과 같음
* 통계적 추론은 본질적으로 불확실성을 수반한다
* 

# 5장 정형 데이터 마이닝

* 통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 데이마이닝은 다양한 수리 알고리즘을 이용해 의미있는 정보를 찾아내는 방법을 통칭
* 종류
	* 정보를 찾는 종류
		* 인공지능
		* 의사결정나무
		* k 평균군집화
		* 연관분석
		* 회귀분석
		* 로짓분석
		* 최근접이웃(nearest neighborhood)
	* 분석대상, 활용목적, 표현방법에 따른 종류
		* 시각화분석
		* 분류
		* 군집화
		* 포케스팅

* 연관규칙 : 데이터 안에 존재하는 항목간의 종속관계 찾음
	* 교차판매, 매장진열, 첨부우편, 사기적발
* 연속규칙 : 연관규칙 + 시간정보으로 고객의 구매이력이 필요
	* 목표마케팅, 일대일 마케팅

#### 데이터마이닝 추진단계
1. 목적설정
	- 전문가가 참여해 목적에 따라 사용할 모델과 필요 데이터 정의
2. 데이터 준비
3. 가공
	- 모델링 목적에 따라 목적 변수 적의
4. 기법적용
5. 검증

#### 데이터 분할
1. 구축용(training data)
2. 검정용(validation data)
3. 시험용(test data)

데이터의 양이 충분하지 않거나 입력 변수에 설명이 부족하면
1. 홀드아웃 방법 : 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용, 학습용과 시험용을 분리하여 사용
2. 교차확인(cross-validation) 방법 : 주어진 데이터를 k개의 하부집단으로 구분하여, k-1개 집단을 학습용 나머지는 검증용으로 사용
 k번 반복 측정한 결과를 평균낸 값을 최종값으로 사용

#### 성과분석
* True Positive : Positive -> Positive 로 예측
* False Negative : Positive -> Negative 로 예측
* False Postive : Negative -> Positive 
* True Negative : Negative -> Negative

1. 정분류율(Accuracy) : TN+TP / 전체
2. 오분류율(Error Rate) : FN+FP / 전체
3. 특이도(Specificity) : TN / TN+FP (전체 Negative 중 똑바로 예측한거)
4. 민감도(Specificity) : TP / TP+FP (전체 Positive 중 똑바로 예측한거)
5. 정확도(Precision) : TP / TP+FP ( Positive로 예측한 것 중 정탐)
6. 재현율(Recall) == 민감도
7. F1 Score : 2*(Precision*Recall)/(Precision+Recall)

**ROCR 패키지로 성과분석**
* ROC Curve(Receiver Operating Characteristic Curve)
	* 가로축 : FPR(false positive rate = 1-특이도, 0을 1로 잘못 예측한 값)
	   세로축 : TPR(민감도,1을 1로 잘 예측한 값)
	* **2진 분류** 성능 평가
	* 왼쪽 상단에 가까울 수록 좋음
	* 면적은 AUROC(Area Under ROC) 값이 클수록 좋음
		* AR = 2*면적 - 100%
```R
pred <- prediction(x.evaluate$probabilities, x.evaluate$Kyphosis)
perf <- performance(pred, “tpr”, “fpr”)
plot(perf, main=“ROC curve”, colorize=T)
```

**이익도표(Lift chart)**
* 분류모형의 성능을 평가하기 위한 척도
* 얼마나 잘 예측됐는지 확인하려고 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표
* 기본 향상도에 비해 반응률이 몇 배나 높은지 계산 -> 향상도(Lift)
* 각 등급은 예측확률에 따라 매겨진 순위, 상위 등급에서는 더 높은 방응률을 보이는것이 좋은 모형
* 등급별로 향상도가 급격하게 변동할수록 좋은 모형

## 분류분석
### 분류분석과 예측분석
**분류분석**
* 클러스터링과 유사하지만, 분류분석은 각 그룹이 정의되어 있음
* 지도학습
* 예) 점수를 통해 내신등급 맞추기, 신용등급 맞추기
* 모델링
	* 신용평가모델, 사기방지모형, 이탈모형, 고객세분화
* 분류기법
	* 회귀분석, 의사결정나무, CART, C5.0, 베이지안 분류, 인공신경망, 지도백터기계(SVM), KNN, 규칙기반의 분류와 사례기반 추론

**예측분석**
* 시계열처럼 시간에 따른 값 두 개만을 이용해 예측
* 한 개의 설명변수
* 예) 수능점수 맞추기, 연 매출 맞추기

**공통점**
* 레코드의 특정 속성의 값을 미리 알아맞히는 점

**차이점**
* 분류 -> 범주형 속성
* 예측 -> 연속형 속성

### 로지스틱 회귀분석
* 반응변수가 범주형
* 각 범주에 속할 확률치 추정
* 모형의 적합을 통해 추정된 확률 -> 사후확률
* exp(B1)의 의미는 나머지 변수(x)가 주어질 때, x1이 한 단위 증가할 때마다 성공(Y=1)의 오즈가 몇 배 증가하는지 나타내는 값
* 오즈비 : 오즈는 성공할 확률이 실패할 확률의 몇 배인지 나타내는 확률이며, 오즈비는 오즈의 비율(성공/실패할 확률)
	* ex) 오즈비가 36이면, 실패할 확률이 성공할 확률의 36배
* 최대우도 추정법(MLE : Maximum Likelihood Estimation)
	* 모수가 미지의 세타인 확률분포에서 뽑은 표본 x들을 바탕으로 세타를 추정하는 기법
	* 우도(likelihood)는 이미 주어진 표본 x들에 비추어봤을 때 모집단의 모수 세타에 대한 추정이 그럴듯한 정도를 말한다
	* 우도는 세타가 전제되었을때 표본x가 등장할 확률인 p(x|세타)에 비례
* glm(종속변수 ~ 독립변수 ~~, familly=binomal, data=)
		
|목적|선형회귀분석  |로지스틱회귀분석|
|--|--|-|
| 종속변수 | 연속형 변수 |(0,1)|
|계수 추정법|최소제곱법|최대우도추정법|
|모형 검정| F,T 검정|카이제곱 검정|

### 의사결정나무
* 비정상 잡음 데이터에 대해 민감함없이 분류가능
* 대용량 데이터도 빠르게 만들 수 있음
* 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향받지 않음
* 수치형, 범주형 모두 가능
* 모형 분류 정확도가 높음
* <단점> 새로운 자료에 대해 과대적합 발생가능성 높음 -> 가지치기필요
* 분류 경계선 부근의 자료값에 대해 오차가 큼
* 설명변수 간의 중요도를 판단하기 쉽지 않음

**분석과정**
1. 성장과정
	* 분리규칙 : 불순도 감소량을 가장 크게
	* 분리기준
		* 이산형 : 카이제곱 통계량 p값, 지니 지수, 엔트로피 지수
		* 연속형 : 분산분석에서 F통계량, 분산의 감소량
	* 정지기준
		* 나무 깊이, 끝마디의 레코드 수의 최소 개수 지정
2. 가지치기
	* 마디에 속하는 자료가 일정 수 이하일 때 분할 정지
	* 비용-복잡도 가지치기를 이용하여 성장시킨 나무를 가지칙ㅔ
3. 타당성 평가
	* 이익도표, 위험도표
4. 해석 및 예측

**불순도 측정 종류**
* 카이제곱 통계량
	* 각 셀에 대한 계산
	* ((실제도수-기대도수)^2/기대도수) 의 합
* 지니지수
	* 열에 대한 계산
	* 노드의 불순도를 나타내는 값
	* 값이 클수록 순수도가 낮음
	* 1-((확률)^2)의 합
	* 지니지수 계산 방법 : 2(P(Left에서 GOOD)P(Left에서 BAD)P(Left)+P(Right에서 GOOD)P(Right에서 BAD)P(Right))
* 엔트로피 지수
	* 열에 대한 계산
	* 무질서 정도
	* 값이 클수록 순수도 낮음
	* -(확률*log확률 의 합)
	* 엔트로피 계산 : 엔트로피(Left)p(Left)+엔트로피(Right)P(Right)

**알고리즘**
* CART(Classification and Regression Tree)
	* 가장 많이씀, 이진분리
	* 불순도의 측도로 출력변수가 범주형 -> 지니지수
	* 연속형 -> 이진분리
	* 입력변수들의 선형결합들 중에서 최적을 분리도 찾을 수 있음
* C4.5, C5.0
	* 다진분리
	* 엔트로피지수 사용
* CHAID(CHi-squared Automatic Interaction Detection)
	* 가지치기를 하지 않고 적당한 크기에서 성장 중지
	* 입력변수는 범주형만
	* 카이제곱 통계량

## 앙상블(Ensemble) 분석

* 주어진 자료로부터 여러 개의 예측모형을 만든 후 조합하여 하나의 최종 예측 모형을 만드는 방법
* 다중 모델 조합, 분류기 조합


***부트스트랩** 은 주어진 자료에서 동일한 크기의 표본을 단순랜덤 복원추출로 뽑은 자료(한번도 선택되지 않은 원데이터 있을 수 있음,38.6%)*

**배깅**
* 주어진 자료에서 여러 개의 부트스트랩 자료를 생성 -> 각 부트스트랩 자료에 예측모형을 만듦 -> 결합 -> 최종 모형 만드는 방법
* **보팅**은 여러 개의 모형으로부터 산출된 결과를 다수결에 의해 최종 결과를 선정하는 과정
* 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정 나무들을 활용
* 훈련자료의 모잡단의 분포를 모르기 때문에 실전 문제에서는 평균예측모형을 구할 수 없다
	* 이를 해결하기 위해 배깅에서는 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상

**부스팅**
* 예측력이 약한 모형들을 결합 -> 강한 예측모형 만들기
* **Adaboost**
	* 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합 -> 최종분류기
	* 단, 가중치의 합은 1
* 훈련오차를 빠르고 쉽게 줄일 수 있음
* 배깅에 비해 예측오차가 향상되어 성능이 뛰어난 경우가 많음

**랜덤 포레스트**
* 배깅, 부스팅보다 더 많은 무작위성을 줌
* 약한 학습기들을 생성한 후 이를 선형결합 -> 최종 학습기 만듦
* 정확도가 좋음
* 설명하기 어려움
* 입력변수가 많으면, 배깅과 부스팅과 비슷하거나 좋은 예측력
* 분류분석에 사용하는 모델 + 과대적합/과소적합의 문제도 해결
* **보험갱신 여부를 고객의 인구통계학적 특성, 보함가입 채널, 상품 종류 등의 정보로 예측할 때 사용**

## 인공신경망 분석
* 신경망은 가중치를 반복적으로 조정하며 학습
* 입력 링크에서 여러 신호를 받아 새로운 활성화 수준 계산 -> 출력 링크로 출력 신호를 보냄

**계산**
* 전이함수(활성화 함수) : 출력을 결정하며 입력 신호의 가중치 합을 계산해 임계값과 비교, 가중치 합이 임계값보다 작으면 뉴력의 출력은 -1, 같거나 크면 +1 출력
	* 시그모이드 함수
		* 0~1사이 확률값
	* softmax 함수
		* 표준화지수 함수, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수
	* Relu 함수
		* 0이하 = 0, 0초과 = x

**입력변수**
모형이 복합하여 입력 자료의 선택에 매우 민감하다.
아래의 조건이 신경망 모형에 적합하다.
* 범주형 변수 : 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주의 빈도가 일정할 때
	* 일정빈도가 아니라면 => 각 범주의 빈도가 비슷하도록 설정
* 연속형 변수 : 입력변수 값들의 범위가 변수간의 큰 차이가 없을 때
	* 분포가 평균을 중심으로 대칭이 아니라면 => 로그변환

**학습모드**
1. 온라인 학습모드
	* 각 관측값을 순차적, 하나씩 신경망투입해서 학습
	* 속도가 빠름
	* 훈련자료가 비정상성과 같이 특이한 성질을 가진 경우 좋음
	* 국소최솟값에서 벗어나기 쉽다
2. 확률적 학습 모드(probabilistic)
	* 온라인 학습 모드와 같으나 신경망에 투입되는 관측값의 순서가 랜덤
3. 배치 학습 모드
	* 전체 훈련자료를 동시에 신경망에 투입

** 은닉층과 노드의 수**
* 수가 많으면 : 가중치가 많아져 과대적합
* 적으면 : 과소적합
* 은닉층 1개 : 범용 근사자이므로 매끄러운 함수를 근사적으로 표현
* 은닉노드 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 사용

**과대 적합 문제**
* 빈번하게 발생
* 해결 : 알고리즘 조기종료(검증오차가 증가하기 시작하면 중지), 가중치 감소 기법(학습할 수록 변경되는 수치를 줄임)


## 군집분석
* 결과는 군집분석 방법에 따라 다를 수 있다
* 군집의 개수나 구조에 대한 가정없이 데이터들 사이의 거리를 기준으로 군집화를 유도
* 마케팅 조사에서 소비자들의 상품구매활동이나 life style에 따른 소비자군을 분류하여 시장 전략 수립등에 활용

*요인분석은 유사한 변수를 함께 묶어주는 것이 목적*
*판별분석은 사전에 집단이 나뉘어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적*

**거리**
관측 데이터 간 유사성이나 근접성으로 -> 군집 판단
**연속형 변수 거리**
* 유클리디안 거리 : 가장 많이 사용, 변수들의 산포 정도 감안X
	* sqrt( (x-y)^2의 합 )
* 표준화 거리 : 표준편차로 척도 변환 후 유클리디안 거리 계산, 표준화하게 되면 왜곡을 피할 수 있음
* 마할라노비스 거리 : 통계적 개념 포함, 변수들의 산포 고려하여 표준화한 거리. 두 벡터 사이의 거리를 표준공분산으로 나눔.
	* sqrt( (x-y)S^(-1)(x-y) ).  , S : 공분산행려ㄹ
* 체비세프 거리 : max|xi-yi|
* 맨하탄 거리 : 블록으로 세는 거리
* 캔버라 거리 : 합 ( |xi-yi| / (xi+yi)
* 민코우스키 거리 : 맨하탄 + 유클리디안

**범주형 변수 거리**
* 자카드 거리  : (합집합 - 교집합) / 합집합
* 자카드 계수 : 교집합 / 합집합
* 코사인 거리 : 1 - A*B / ||A|| * ||B|| (*는 행렬 곱)
* 코사인 유사도 : A*B / ||A|| * ||B||

### 계층적 군집분석
* n개의 군집으로 시작해 점차 군집 개수를 줄이는 방법
* 합병형 방법, 분리형 방법
* 최단연결법 : 거리 계산 시 최단거리 계산
* 최장연결법 : 최장거리로 계싼
* 평균연결법 : 평균을 거리로 계산
* 와드연결법 : 군집내 편차들의 제곱합, 군집 간 정보의 손실을 최소화
* 군집화 : 덴드로그램으로 군집선정

### 비계층적 군집분석
* k-평균 군집분석
	* 주어진 데이터를 k개의 클러스터로 묶음
	* 각 클러스터와 거리 차이의 분산을 최소화
	* 원하는 군집 개수와 seed 정하기 -> seed를 중심으로 군집 형성 -> 각 데이터를 거리가 가까운 seed가 있는 군집으로 분류 -> seed값 다시 계산(모든 개체가 군집으로 할당될 때까지 반복)
	* 거리 계산을 통해 군집화 -> **연속형 변수**에 활용 가능
	* 초기 중심값의 선정에 따라 결과가 달라짐
	* 탐욕적 알고리즘이므로 최적이라는 보장은 없음
	* 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있음
	* 다양한 형태의 데이터에 적용가능


## 연관분석
* 연관분석 = 장바구니분석 = 서열분석(a를 산다음에 b를 산다)

**측도**
1. 지지도(support)
	* 전체 거래 중 항목 A,B를 동시에 포함
	* 합집합 / 전체
2. 신뢰도(confidence)
	* A를 포함한 거래 중 A,B를 같이 포함할 확률
	* A를 산사람 중 B를 살 확률
	* 합집합 / P(A) == 지지도/P(A)
3. 향상도(Lift)
	* A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 B의 구매확률의 증가 비, A -> B는 A와 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다.
	* A를 샀을 때 B를 살 확률이 얼마나 증가하는가
	* P(B|A) / P(B) = P(A 합 B) / P(A)P(B) = A와 B가 동시에 포함된 거래 수/ (A를 포함하는 거래수 * B를 포함하는 거래 수) = 신뢰도/P(B)

## SOM
* 비지도학습 군집분석
* 
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjAyMDY2OTk3LDE1NTMwNjQ1MjIsNjY1OT
AwNDQ0LDEyMDA2NzMwMzIsLTE5ODMzODc0MzksNjQ1MzgwNzQ0
LC00MTcyNTI5NzAsNTYwOTI2MTYwLC0xMjAxOTA4MDIzLC0yMD
k0MDk2MzM0LC0yMDMzNTYzNzcwLC0yMDg4NzQ2NjEyXX0=
-->