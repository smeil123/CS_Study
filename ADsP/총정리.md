# 데이터 이해

* 데이터베이스
	* 특징 : 통합된 데이터, 저장된 데이터, 공용 데이터, 운영 데이터
	* 무결성 : 디비에서 여러 제한을 두어 데이터의 정확성을 보증
* OLTP(On-Line Transaction Processing)
	* DB <-> 1개의 호스트 <-> 다수의 단말 : 우리가 흔히 아는 DB사용방법
	* 디비를 수시로 갱신
	* 주문 입력 시스템, 재고 관리
* OLAP(On-Line Analytical Processing)
	* 정보 위주 분석 처리, 데이터 조회 위주
	* 다양한 비즈니스 관점에서 다차원 데이터에 접근해 활용
	* 제품의 판매 추이, 구매 성향 파악
* CRM(Customer Relationship Management) : 고객 맞춤 마켓팅이 목적
* SCM(Supply Chain Management) : 공급망 관리, 원재료 ~ 제품배달 까지를 거래처들과 실시간 공유
* ERP(Enterprise Resource Planning) : 분야별 독립적으로 운영되던 시스템의 경영지원을 하나의 통합 시스템으로 재구축, 생선성을 극대화하려는 경영 혁신 기법
* BI(Business Intelligence) : 기업이 보유하고 있는 수많은 데이터를 정리하고 분석해 기업의 의사결정에 활용하는 일련의 프로세스
* EAI : 정보를 중앙 집중적으로 토
* 상호 연결된 정보 패턴을 이해하고 예측 : 지식
* 데이터 사이이언티스트 soft skill
	* 창의적 사고, 커뮤니케이션 기술, 호기심, 스토리텔링
* 빅데이터 분석에 경제성 제공 결정적 기술 : **클라우드 컴퓨팅**
* 빅데이터 분석 출현 배경
	* M2M, IoT와 같은 통신 기술의 발전
	* 하둡 등 분산처리 기술 발전
	* 트위터, 페이스북 등 SNS의 급격한 확산
* 인문학 열풍의 원인
	* 단순 세계화 -> 복잡한 세계화
	* 제품 생산 -> 서비스
	* 생산 -> 시장 창조
* 데이터 분석 수준 진단
	* 분석업무, 분석 인력/조직, 분석기법, 분석 데이터, 분석 문화, 분석 인프라
* 비즈니스 모델 캔버스는 9가지 블록을 단순화하여 업무, 제품, 고객단위로 문제를 발굴하고 이를 관리하는 규제와 감사, 지원 인프라 영역으로 나눠 분석 기회를 도출한다.
	* 규제와 감사
	* 업무 > 제품 < 고객
	* 지원인프라
* 책임 원칙 훼손 : 일어나지 않은 일을 예상된다고 체포함
* 렌즈 : 우리가 확인하기 힘들었던 부분을 알려줌(ex. Ngram Viewer)
* 빅데이터 가치 산정이 어려운 이유
	* 데이터 활용 방식 : 재사용, 재활용, 다목적 개발
	* 새로운 가치 창출
	* 분석기술 발전
* 분석 기반 경영이 도입되지 못하는 이유
	* 기존 관행을 그냥 따를 뿐 시도하지 않음
	* 경영진의 직관적인 결정이 재능이라고생각
	* 분석할 사람이 없음
	* 아이디어보다 낸사람에 더 관심
* 데이터 분석 테크닉
	* 소셜 네트워크 분석 - 최근 핀테크 기업에서 대출 제공시 활용
	* 기계학습 - 대규모 데이터를 처리할 때 상당한 분석 인프라와 시간 소요
	* 한국어의 경우 언어 특성으로 인해 감성 분석이 상대적으로 어려움
	* 개인 신용도 평가에 분류/예측 모형이 가장 많이 활용 ~~연관규칙말고~~
	* 택배차량을 어떻게 배치하는 것이 비용측면에서 효율적인가 - 유전자 알고리즘
* 암묵지와 형식지 상호작용 : 내면화, 공통화, 표출화, 연결화

# 데이터 분석 기획

[분석의 대상 (What)]
아는것-----------------모르는것
*Optimization--------*Insight				아는것	[분석의 방법(How)]
*Solutin---------------*Discovery			모르는것
* 분석 방법론 : 절차, 방법, 도구와 기법, 템플릿과 산출물
* 데이터 준비 단계 <-> 데이터 분석 단계에 제일 피드백 많음
* 빅데이터 특징 4V
	* 투자비용 : Volume, Variety, Velocity
	* 비즈니스 효과 : Value
* 프레이밍 효과 : 문제의 표현 방식에 따라 동일한 사건이나 상황임에도 불구하고 개인의 판단이나 선택이 달라질 수 있는 현상
* CRISP-DM 분석 방법론
	* 계층적 프로세스 모델로 4개 레벨
		1. Phases
		2. Generic Tasks
		3. Specialized Tasks
		4. Process Instances(프로세스 실행)
	* 프로세스는 6단계, 단계간 피드백을 통해 완성도를 높임
		1. 업무 이해
		2. 데이터 이해
		3. 데이터 준비
			* 잡음, 이상치, 결측치를 식별하여 분석용 데이터셋을 선택하고 분석에 필요한 변수를 선정하는 단계
		4. 모델링
			* 모델링 기법 선택, 모델 테스트 계획 설계, 모델 작성, 모델 평가
		5. 평가
		6. 전개 : 실 업무에 적용, 요지보수 계획
	* 평가를 통해 모델에 대한 평가 등을 파악하며, 비지니스에 대한 이해가 부족해 모형 개발이 잘못되었을 때 다시 비지니스 이해로 돌아감
* 프로토타이핑 접근법 : 사용자가 요구사항이나 데이터를 정확히 규정하기 어렵고 데이터 소스도 명확히 파악하기 어려운 상황에서 일단 분석을 시도해보고 그 결과를 확인하면서 **반복적으로 개선**해 나가는 방법
	* 필요한 상황
		* 문제에 대한 인식 수준이 낮을 때
		* 필요 데이터 존재 여부가 불확실할때
		* 데이터 사용 목적의 가변성
* 통찰(Insight) 유형 : 분석대상이 뭔지 잘 모르면서 기존 분석 방법으로 분석
* Solution 유형 : 분석 대상은 명확하지만 분석 방식이 명확하지 않은 경우
* 성공적인 분석을 위한 고려요소
	* 분석 데이터에 대한 고려
	* 활용 가능한 유즈케이스 탐색
	* 장애 요소에 대한 사전 계획 수립
	* ~~원점에서 솔루션 탐색~~
* 고객 니즈의 변화
	* 고객, 채널, 영향자들
	* ~~대체제~~
* 분석과제로 확정되면 풀에서 제외된다
* 기능형 : 별도의 분석 조직이 없고 업무부서에서 분석 수행
* Why == 하향식 접근 : 분석해야할 대상이 명확하면
* What == 상향식 접근 : 사물을 있는 그대로 인식, 비지도 학습방법에 의해 수행
* 하향식의 타당성 평가 : 데이터 존재, 분석 시스템 환경, 분석 역량
* 분석과제 주요 관리 영역
	* Data size : 분석하고자 하는 데이터양을 고려한 관리방안
	* Data Complexity : 데이터에 잘 적용될 수 있는 분석모델 선정
	* Speed : 분석 모델의 성능 및 속도를 고려한 개발 테스트 수행
	* Analytic&Complexity : 해석이 가능하면서도 정확도를 올릴 수 있는 최적 모델을 찾는 방안을 모색해야함
		* 모델의 정확도와 복잡도는 트레이드 오프 관계 존재
		* 분석 모델이 보잡할 수록 정확도는 올라가지만 해석이 어려워지는 단점 존재하여 기준점을 사전에 정의해야함
	* Accuracy&Precision
		* Accuracy : 모델과 실제 값 사이의 차이는 정확도
		* Precision : 모델을 지속적으로 반복했을 때의 편차의 수준으로써 일관적으로 동일한 결과를 제시한다는 것의미
		* 둘 관게도 트레이드 오프가 됨
* 빅데이터 거버넌스
	* 데이터 분석의 지속적인 적용과 확산을 위한 거버넌스 체계의 구성 요소
		* Processs
		* System
		* Oranization(분석 기획)
		* Data
		* Human Resource(교육, 마인드 육성)
	* 데이터 표준화 : 데이터 표준 용어 설정, 명명 규칙 수립, 메타 데이터 구축, 데이터 사전 구축 등의 업무로 구성된 데이터 거버넌스 체계
	* 데이터 관리 체계 : 데이터 정합성 및 활용의 효율성을 위하여 표준 데이터를 포함한 메타 데이터와 데이터 사전의 관리 원칙을 수립
	* 데이터 저장소 관리 : 메타데이터 및 표준데이터를 관리하기 위한 전사 차원의 저장소 구축
	* 표준화 활동 : 데이터 거버넌스 체계를 구축한 후 표준 준수 여부를 주기적으로 점검하고 모니터링
	* 데이터 관리 체계 : 메타데이터 관리, 데이터 사전관리, 데이터 생명주기 관리
	* ERD는 운영 중인 데이터베이스와 이ㄹ치하기 위하여 철저한 변경관리 필요
	* 데이터 수명 주기와 품질관리 모두 중요
	* 저장소는 데이터 관리 체계 지원을 위한 **워크플로우** 및 관리용 응용소프트웨어를 지원하고 관리대상 시스템과의 인터페이스를 통한 통제가 이루어져야한다.
	* 데이터 구조 변경에 따른 **사전 영향 평가**도 수행되어야 효율적인 활용이 가능하다
* BI(Business Intelligence)와 비교하여, 빅데이터 키워드 설명
	* Information, Ad hoc Report, Alert, Clean Data
* 분석 프로젝트 관리
	* 프로젝트관리 지침(KSA ISO 21500:2013) 활용
	* 품질 평가를 위해 SPICE 활용
	* 최종 결과물에 따라 관리 방식에 차이 있음
	* Time boxing 기법과 같은 방향으로 일정관리
	* ~~데이터 수집에 대한 철저한 통제 관리 필요~~
* Accuracy와 Precision
	* 활용적인 측면 : Accuracy, 안정성 측면 : Precision
	* Accuracy 는 모델과 실제 값과의 차이를 평가하는 정확도
	* Precision 모델을 지속적으로 반복했을 때의 편차의 수준, 일관적으로 동일한 결과를 제시한다는 의미
	* Trade-Off관계
* 분석 유즈 케이스 : 문제에 대한 상세한 설명, 해결했을 때 발생하는 효과를 명시함으로써 향후 데이터 분석 문제로의 전환 및 적합성 평가에 활용하도록 함
* 이탈고객 : 더 이상 상품과 서비스를 사용하지 않고 경쟁사와 거래하는 고객
* 디자인 사고 : 상향식 접근 방식의 발산단계와 도출된 옵션을 분석하고 검증하는 하향식 접근 방식의 수렴단계를 반복하여 과제를 발굴하는 방법
* 장기적인 마스터 플랜 방식
	* Accuracy & Deploy
	* 우선순위 고려요소 : 전략적 중요도, 비즈니스 성과/ROI, 실행 용이성
* 과제 중심적인 접근 방식	
	* Quick-Win
	* Problem Solving
	* Speed & Test
* Service Analytics : 분산처리하지 않는다
* 데이터 가치 재해석을 위해선
	* 내부의 노하우
	* 인프라와 파트너사의 정보
	* 네트워크를 왈용한 정보 확보
* 비즈니스 모델 캔버스
	* 채널 :  영업 사원, 직판 대리점과 홈페이지 등의 자체적으로 운영하는 채널뿐만 아니라 최종 고객에게 상품/서비스를 전달하는데 있어서 가능한 경로에 존재하는 채널도 포함하고 있다. ~~하지만, 구매 고객에 대한 애프터서비스 제공은 관련없음~~ 

#  데이터 분석

# 1장 데이터 분석 개요

### 모델링 성능 평가 기준
데이터마이닝 -> 정확도, 정밀도, 디텍트레이트, lift
시뮬레이션 -> throughput, average wating time, average queue length, time in system

### 공간분석(GIS)

### 탐색적 데이터 분석(EDA)
* 데이터를 시각화하면 이상점(outlier) 식별이 잘 된다
* 알고리즘의 학습 성능에는 전적으로 데이터의 품질과 데이터에 담긴 정보량에 달려있다
* 4가지 주제
	* 잔차계산 : 주경향에서 얼마나 벗어났는지
	* 저항성의 강조 : 부분 변동에 크게 영향을 받지 않도록
	* 자료변수의 재표현
	* 그래프를 통한 션시성


#### 표본추출 방법
* 단순랜덤추출법
* 계통추출법
* 층화추출법

#### 연속형 확률 변호
* 균일 분포 : 모든 확률 변수 X가 균일한 확률을 가짐
* 정규분포 : 평균이 m이고, 표준편차가 시그마인 x의 확률밀도함수
	* 표준편차가 클 경우 퍼져보이는 그래프
	* 표준정규분포 : 평균이 0이고 표준편차가 1
* 지수분포 : 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포
* t -분포 : 0을 중심으로 좌우가 동일한 분포
	* 표본의 크기가 적을 때 : 표준 정규분포를 위에서 눌러눈거같음
	* 표준이 커짐(30개 이상) : 표준정규분포와 거의 같은 분포
	*  정규 분포의 평균을 측정할 때 주로 사용되는 분포로 두 집단의 평균 차이 검증
* 카이제곱 분포
	* 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용
	* 두 집단간의 동질성 검정에 사용
	* 범주형 자료에 대해 얻어진 관측값과 기대값의 차이를 보는 적합성 검정에 활용
* F-분포
	* 두 집단간 **분산**의 동일성 검정에 사용
	* 자유도를 2개 가지고 있으며. 자유도가 커지면 정규분포에 가까워짐

#### 이산형 확률 분포
> 이산형 확률 변수 : 0이 아닌 확률값을 갖는 확률 변수를 셀 수 있는 경우

* 베르누이 확률분포 : 개별 사건이 2개만!, 각 사건이 성공할 확률이 일정하고 전후사건에 독립적인 특수한 상황
* 이항분포 : 베르누이 시행을 n번 반복했을 때 k번 성공할 확률
	* 성공할 확률p가 0이나 에 가깝지 않고 n이 충분히 크면 **정규분포**에 가까워짐
* 기하분포 : 성공확률이 p인 베르누이 시행에서 첫번째 성겅이 있기까지 x번 실패할 확률
* 다항분포 : 이항분포 확장, 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률분포
* 포아송 분포 : 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포
	* 예) 책에 오타가 5page 당 10개씩 나온다고 할 때, 한 페이지에 오타가 3개 나올 확률

# 2장 R프로그래밍 기초

* 벡터에는 한가지 자료형만 넣을 수 있음
	* 만약 다른 숫자 + 문자가 들어가면 숫자도 문자형이 된다
* c(2, 4, 6, 8) + C(1, 3, 5, 7, 9)
	* 경고 메시지와 함께 결과 출력 (3 7 11 15 11)
* merge하면 inner join 된다
```R
z = c(1:3, NA)
z==NA
# NA NA NA NA
```
* 숫자형 행렬에서 원소 중 하나를 문자형으로 변경 ==> 해당 행렬의 모든 원소가 문자형으로 바뀜
* 행렬을 as.vector함수에 입력하면 1열부터 차례로 원소를 나열하는 벡터 형성
```R
x = matrix(c(1,2,3,4), nrow=2)
# 1 3
# 2 4
as.vector(x)
# 1 2 3 4
```
```R
“+”(2,3) # 5
```
```R
y = c(1,2,3,NA)
# 3,6,9,NA
```R
x <- c(1:5)
y <- seq(10,50,10)
xy <- rbind(x,y)
# 1 2 3 4 5
# 10 20 30 40 50
```
```R
dim(m1) <- c(4,5) # 4x5행렬
apply(m1, 1, sum)# 행의 합
apply(m1, 2, sum)# 열의 합
lapply(m1,sum)# 각 셀의 합, 총20개의 해
```
```r
x<-1:100
sum(x>50) # 50
```
```r
subset(test, subset=(학과==경영학과))
sqldf()
```

# 3장 데이터마트
* reshape 패키지
	* melt, cast
	* 변수를 조합하 변수를 만들고, 결합해 요약변수와 파생변수를 쉽게 생성
```r
cast(md, id+variable~time)
sqm <- melt(airquality,id=c(“Month”,”Day”), na.rm=TRUE)
```
* 요약변수
	* 데이터마트에서 기존적인 변수
	* 재활용성이 높음
	* 다양한 모델을 개발해야 하는 경우, 효율적으로 사용
* 파생변수
	* 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 부여
	* 주간적, 논리적 타당성을 맞춰야함
* 결측치
	* 칸이 비어있는 경우 결측치 여부는 알기 쉬움
	* 관측치가 있지만 실상은 default값이 기록된 경우에도 의미가 있음
	* 결측치가 20%이상인 경우에는 해당 변수를 제거하고 분석
	* 다중대치법 : m번 대치를 통해 m개의 가상적 완전 자료를 만들어 분석, 대치 -> 분석 => 결합
	* complete.cases() : 결측치가 없으면 T, 있으면 F
	* is.na() : 결측값이 NA인지 확인
	* centrallmputation() : NA값에 가운데 값으로 대치, 숫자는 최빈값으로
	* knnImputation() : NA값을 knn을 사용해 주변 이웃의 거리를 고려야혀 대치
	* rfImpute() : 랜덤 포레스트 모양의 경우, 결측값이 있으면 에러를 발생하기 때문에 랜덤포레스트 패키지에서 NA결측값을 대치하도록 하는 함수

* 이상치
	* IQR=Q3-Q1, Q1-1.5*IQR<x<Q3+1.5*IQR
	* 평균으로 부터 3*표준편자 넘어간 값  => ESD(Extreme Studentized Deviation) 
	* 상자그림으로 확인
	* 이상치 제거여부는 실무자를 통해서 결정
	* 군집분석으로 다른 데이터들과 거리상 멀리 떨어진 데이터를 이상치로 봄
	* 데이터 측정과정이나 입력과정에서 잘못된 이상치도 의미있을 수 있어 바로 삭제하지 않음
	* 설명변수의 관측치에 비해 종속변수의 값이 상이한 값도 이상치
	* 환경파괴에는 이상치를 적용하기 어려움

* plyr패키지
	* multi-core를 사용하여 반복문을 사용하지 않고도 매우 간단하고 빠르게 데이터 처리
```r
# feed별로 weight평균값 구하기
ddply(chickwts, ~feed, summarize, groupmean=mean(weight)) 
```
# 4장 통계분석
### 표본추출 방법
* 단순랜덤 추출법
* 계통추출법 : 구간을 나누고 구간별로 K번째 자료를 추출 
* 집락추출법 : 군집을 구분하고 군집별로 단순랜덤 추출
* 층화추출법 : **이질적인 원소들로 구성된 모집단에서**각 계층을 고루 대표할 수 있는 표본 추출

#### 척도종류
* 범주형자료
	* 명목척도 : 카테고리, 집단(성별, 시/도)
	* 순서척도 : 서열관계 ( 만족도, 학년, 신용등급)
* 수치형자료
	* 구간척도(등간척도) : 속성의 양을 측정, 간격이 의미가 있음(온도, 지수) , 더하기 빼기는 가능하나 곱하기 나누기 불가
	* 비율척도 : 차이에 대한 비율이 의미를 가짐, 절대적 기준인 0이 존재, 칙연산 가능(무게, 나이, 거리)

### 확률
* 두 사건 A,B가 독립이면, P(B) = P(A|B)
* 통계적 추론은 제한된 표본을 바탕으로 모집단에 대한 일반적인 결론을 유도하려하는 시도이므로 본질적으로 불확실성을 수반
* 구간추정은 실제 모집단의 모수는 신뢰구간에 포함하는지는 모른다

### 표본조사
* 표본오차 : 모집단을 대표할 수 있는 표본 단위들이 조사대상으로 추출되지 못함으로 발생
* 표본편의(bias)
	* 모수를 작게 또는 크게 할 때 추정하는 것과 같이 표본추출방법에서 기인하는 오차, **확률화**에 의해 최소화하거나 없앨 수 있다.
		* 표본추출 과정에서 특정 대상이 다른 대상에 비해 우선적으로 추출될 때 발생
* 확률화 : 모집단으로부터 편의되지 않은 표본을 추출하는 절차를 의미하며 확률화 절차에 의해 추출된 표본을 확률표본이라한다
* 비표본오차 : 모든 부주의나 실수, 알 수 없는 원인 등 모든 오차. 조사 대상이 증가하면 오차도 커짐 

### 통계적 추론
* 정규모집단으로부터 n개의 단순임의 추출한 표본의 분산은 자유도가 n-1인 카이제곱 분포를 따름
* 이 표본에 의한 분산비 검정은 두 표본의 분산이 동일한지를 비교하는 검정으로 검정통계랑은 F분포를 따름
* 귀무가설이 사실일 때, 관측된 검정통계량의 값보다 대립가설을 지지하는 방향으로 검정통계량이 나올 확률은 p-value
* 검정력이란 대립가설이 맞을 때 그것을 받아들이는 확률을 의미
* 모수적방법
	* 
* 비모수적 검정
	* 모집단의 분포에 대해 아무런 제약을 가하지 않음
	* 관측된 자료가 특정 분포를 따른다고 가정할 수 없을 때 사용
	* 분포의 형태에 대해 가설을 설정
	* 관측값의 절대적 크기에 의존하지 않는 관측값들의 순위, 차이의 부호등을 이용
		* 예시) 윌콕슨의 순위합검증, 만-위트니의 U검정, 스피어만의 순위상관계수,, ~~자기상관검증~~
* 표본을 도표화하여 분포 개형 파악
	* 히스토그램 : 도수분포표를 이용하여 분포를 나타냄, 수평축 위에 계급구간을 표시하고 그 위로 각 계급의 상대도수에 비례하는 직사각형
	* 줄기잎그림 : 각 데이터의 점들을 구간단위로 요약, 계산량이 작음
	* 산점도
	* 파레토그림 : 명목형 자료에서 중요한 소수를 찾는데 유용

###
* 제1종 오류 : Ho이 사실일 때, 사실이 아니라고 판정
* 제2종 오류 : Ho가 사실이 아닐 때, 사실이라고 판정

### 상관분석
두 변수 간의 관계의 정도를 파악하는 분석 방법
|구분|피어슨|스피어만|
|--|--|-|
|개념  |등간척도  |서열척도|
|특징|연속형 변수, 정규성가정, 많이 사용|순서형 변수, 비모수적 방법|
|상관계수|피어슨 r(적률상관계수)|순위상관계수(p,로우)|
* 피어슨 계산 : ( (X-X평균) * (Y-Y평균 ) 의 합)/(X-X평균)의 제곱합 * (Y-Y평균)의 제곱합
* t 검정통계량을 통해 얻은 p-value값이 0.05이하인 경우, 대립가설을 채택하게 되어 상관계수를 활용할 수 있음
* 변수간의 분산은 알 수 없음
* 상관계수간의 유의성은 판단할 수 없음

### 회귀분석
* 가정 : 독립성, 선형성, 정규성, 등분산성, 비상관성
	* 정규성 : 잔차항의 정규분포
	* 등분산성 : 산점도를 그렸을 때 나팔모양이면 오차의 분산이 예측치가 커짐에 따라 커지거나 작아지고 있음을 의미하며 등분산 가정이 무너짐을 의미
* 다중회귀모형이 통계적으로 유의미한지 : **F통계량** 확인
* 단순회귀  : **t통계량** 확인
* 결정계수(R2)
	* 총 변동 중에서 설명되는 변동이 차지하는 비율
	* 입력변수가 증가하면 결정계수도 증가
	* 수정된 결정계수는 유의하지 않은 독립변수들이 회귀식에 포함되었을 때 그 값이 감소
	* SSR / SST = (SST-SSE) / SST
* 데이터의 정규성 확인 방법
	* 히스토그램, q-q plot, shaprio-Wilik test
* lasso 회귀모형
	* 회귀계수들의 절대값의 크기가 클수록 패널티 부여
	* 자동적으로 변수선택을 하는 효과
	* Lambda값으로 패널티 조정
	* L1 패널티 사용
* 전진선택법, 후진제거법
* 최소제곱법으로 오차를 줄임

### 시계열 분석
* 정상시계열 : 시점에 상관없이 시계열의 특성을 일정함
* 자기회귀모형 : 자기 자신의 과거 값을 사용하여 설명하는 모형
* 분해시계열 : 상시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법
* 시간그래프 그리기 -> 추세와 계절성 제거 -> 잔체 예측 -> 모델 적합 -> 미래예측
* 추세분석 : 장기적으로 변해가는 큰 흐름
* 계절변동 : 짧은 기간 동안의 주기적인 패턴
* 순환변동 : 알려지지 않은 변동
* 불규칙변동 : 불규칙하게 변동하는 급격한 환경변화, 천재지변
### 교차분석
2개 이상의 변수를 결합하여 자료의 빈도를 살펴보는 기법
* 교차분석의 모두 범주형 변수여야 사용가능
* 두 변수들 간의 독립성 검정 가능
* 기대빈도가 5미만인 셀의 비율이 20%를 넘으면 카이제곱분보에 근사하지 않음, 표본을 늘리거나 변수의 수준을 합쳐 셀의 수를 줄여야함

### 주성분분석
* 다변량 자료를 저차원 그래프로 표시하여 이상치 탐색에 사용
* 회귀분석에서 다중공선성의 문제 해결을 위해 활용
* Scree graph를 이용하는 방법은 고유값의 크기순으로 산점도를 그린 그래프에서 감소하는 추세가 원만해지는 지점에서 1을 뺀 개수를 주성분의 개수로 함
* 평균 고유값방법은 고유값들의 평균을 구한 후 고유값이 평균값 이상이 되는 주성분을 **설정**하는 방법

##
* A,B가 독립이면 각 조건부확률은 독립확률과 같음
* 통계적 추론은 본질적으로 불확실성을 수반한다


# 5장 정형 데이터 마이닝

* 통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 데이마이닝은 다양한 수리 알고리즘을 이용해 의미있는 정보를 찾아내는 방법을 통칭
* 종류
	* 정보를 찾는 종류
		* 인공지능
		* 의사결정나무
		* k 평균군집화
		* 연관분석
		* 회귀분석
		* 로짓분석
		* 최근접이웃(nearest neighborhood)
	* 분석대상, 활용목적, 표현방법에 따른 종류
		* 시각화분석
		* 분류
		* 군집화
		* 포케스팅

* 연관규칙 : 데이터 안에 존재하는 항목간의 종속관계 찾음
	* 교차판매, 매장진열, 첨부우편, 사기적발
* 연속규칙 : 연관규칙 + 시간정보으로 고객의 구매이력이 필요
	* 목표마케팅, 일대일 마케팅

#### 데이터마이닝 추진단계
1. 목적설정
	- 전문가가 참여해 목적에 따라 사용할 모델과 필요 데이터 정의
2. 데이터 준비
3. 가공
	- 모델링 목적에 따라 목적 변수 적의
4. 기법적용
5. 검증

#### 데이터 분할
1. 구축용(training data)
2. 검정용(validation data)
3. 시험용(test data)

데이터의 양이 충분하지 않거나 입력 변수에 설명이 부족하면
1. 홀드아웃 방법 : 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용, 학습용과 시험용을 분리하여 사용
2. 교차확인(cross-validation) 방법 : 주어진 데이터를 k개의 하부집단으로 구분하여, k-1개 집단을 학습용 나머지는 검증용으로 사용
 k번 반복 측정한 결과를 평균낸 값을 최종값으로 사용

#### 성과분석
* True Positive : Positive -> Positive 로 예측
* False Negative : Positive -> Negative 로 예측
* False Postive : Negative -> Positive 
* True Negative : Negative -> Negative

1. 정분류율(Accuracy) : TN+TP / 전체
2. 오분류율(Error Rate) : FN+FP / 전체
3. 특이도(Specificity) : TN / TN+FP (전체 Negative 중 똑바로 예측한거)
4. 민감도(Specificity) : TP / TP+FN (전체 Positive 중 똑바로 예측한거)
5. 정확도(Precision) : TP / TP+FP ( Positive로 예측한 것 중 정탐)
6. 재현율(Recall) == 민감도
7. F1 Score : 2*(Precision*Recall)/(Precision+Recall)
	* Fp 지표에서 p = 2 : 재현율에 2배만큼의 가중치를 부여하여 조화 평균

**ROCR 패키지로 성과분석**
* ROC Curve(Receiver Operating Characteristic Curve)
	* 가로축 : FPR(false positive rate = 1-특이도, 0을 1로 잘못 예측한 값)
	   세로축 : TPR(민감도,1을 1로 잘 예측한 값)
	* **2진 분류** 성능 평가
	* 왼쪽 상단에 가까울 수록 좋음
	* 면적은 AUROC(Area Under ROC) 값이 클수록 좋음
		* AR = 2*면적 - 100%
```R
pred <- prediction(x.evaluate$probabilities, x.evaluate$Kyphosis)
perf <- performance(pred, “tpr”, “fpr”)
plot(perf, main=“ROC curve”, colorize=T)
```

**이익도표(Lift chart)**
* 분류모형의 성능을 평가하기 위한 척도
* 얼마나 잘 예측됐는지 확인하려고 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표
* 기본 향상도에 비해 반응률이 몇 배나 높은지 계산 -> 향상도(Lift)
* 각 등급은 예측확률에 따라 매겨진 순위, 상위 등급에서는 더 높은 방응률을 보이는것이 좋은 모형
* 등급별로 향상도가 급격하게 변동할수록 좋은 모형

## 분류분석
### 분류분석과 예측분석
**분류분석**
* 클러스터링과 유사하지만, 분류분석은 각 그룹이 정의되어 있음
* 지도학습
* 예) 점수를 통해 내신등급 맞추기, 신용등급 맞추기
* 모델링
	* 신용평가모델, 사기방지모형, 이탈모형, 고객세분화
* 분류기법
	* 회귀분석, 의사결정나무, CART, C5.0, 베이지안 분류, 인공신경망, 지도백터기계(SVM), KNN, 규칙기반의 분류와 사례기반 추론

**예측분석**
* 시계열처럼 시간에 따른 값 두 개만을 이용해 예측
* 한 개의 설명변수
* 예) 수능점수 맞추기, 연 매출 맞추기

**공통점**
* 레코드의 특정 속성의 값을 미리 알아맞히는 점

**차이점**
* 분류 -> 범주형 속성
* 예측 -> 연속형 속성

### 로지스틱 회귀분석
* 반응변수가 범주형
* 각 범주에 속할 확률치 추정
* 모형의 적합을 통해 추정된 확률 -> 사후확률
* exp(B1)의 의미는 나머지 변수(x)가 주어질 때, x1이 한 단위 증가할 때마다 성공(Y=1)의 오즈가 몇 배 증가하는지 나타내는 값
* 오즈비 : 오즈는 성공할 확률이 실패할 확률의 몇 배인지 나타내는 확률이며, 오즈비는 오즈의 비율(성공/실패할 확률)
	* ex) 오즈비가 36이면, 실패할 확률이 성공할 확률의 36배
* 최대우도 추정법(MLE : Maximum Likelihood Estimation)
	* 모수가 미지의 세타인 확률분포에서 뽑은 표본 x들을 바탕으로 세타를 추정하는 기법
	* 우도(likelihood)는 이미 주어진 표본 x들에 비추어봤을 때 모집단의 모수 세타에 대한 추정이 그럴듯한 정도를 말한다
	* 우도는 세타가 전제되었을때 표본x가 등장할 확률인 p(x|세타)에 비례
* 로짓변환 : log(어떠한 일이 일어날 확률 / 일어나지 않을 확률 ) 해서 시그모이드 함수로 변환
* 계수가 5.1 이라면, 종속변수가 한단계 증가함에 따라 목적변수가  1-> 2로 증가할 비율이 exp(5.1) 약 170배 
* glm(종속변수 ~ 독립변수 ~~, familly=binomal, data=)
		
|목적|선형회귀분석  |로지스틱회귀분석|
|--|--|-|
| 종속변수 | 연속형 변수 |(0,1)|
|계수 추정법|최소제곱법|최대우도추정법|
|모형 검정| F,T 검정|카이제곱 검정|

### 의사결정나무
* 비정상 잡음 데이터에 대해 민감함없이 분류가능
* 대용량 데이터도 빠르게 만들 수 있음
* 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향받지 않음
* 수치형, 범주형 모두 가능
* 모형 분류 정확도가 높음
* <단점> 새로운 자료에 대해 과대적합 발생가능성 높음 -> 가지치기필요
* 분류 경계선 부근의 자료값에 대해 오차가 큼
* 설명변수 간의 중요도를 판단하기 쉽지 않음

**분석과정**
1. 성장과정
	* 분리규칙 : 불순도 감소량을 가장 크게
	* 분리기준
		* 이산형 : 카이제곱 통계량 p값, 지니 지수, 엔트로피 지수
		* 연속형 : 분산분석에서 F통계량, 분산의 감소량
	* 정지기준
		* 나무 깊이, 끝마디의 레코드 수의 최소 개수 지정
2. 가지치기
	* 마디에 속하는 자료가 일정 수 이하일 때 분할 정지
	* 비용-복잡도 가지치기를 이용하여 성장시킨 나무를 가지칙ㅔ
3. 타당성 평가
	* 이익도표, 위험도표
4. 해석 및 예측

**불순도 측정 종류**
* 카이제곱 통계량
	* 각 셀에 대한 계산
	* ((실제도수-기대도수)^2/기대도수) 의 합
* 지니지수
	* 열에 대한 계산
	* 노드의 불순도를 나타내는 값
	* 값이 클수록 순수도가 낮음
	* 1-((확률)^2)의 합
	* 지니지수 계산 방법 : 2(P(Left에서 GOOD)P(Left에서 BAD)P(Left)+P(Right에서 GOOD)P(Right에서 BAD)P(Right))
* 엔트로피 지수
	* 열에 대한 계산
	* 무질서 정도
	* 값이 클수록 순수도 낮음
	* -(확률*log확률 의 합)
	* 엔트로피 계산 : 엔트로피(Left)p(Left)+엔트로피(Right)P(Right)

**알고리즘**
* CART(Classification and Regression Tree)
	* 가장 많이씀, 이진분리
	* 불순도의 측도로 출력변수가 범주형 -> 지니지수
	* 연속형 -> 이진분리
	* 입력변수들의 선형결합들 중에서 최적을 분리도 찾을 수 있음
* C4.5, C5.0
	* 다진분리
	* **엔트로피지수** 사용
* CHAID(CHi-squared Automatic Interaction Detection)
	* 가지치기를 하지 않고 적당한 크기에서 성장 중지
	* 입력변수는 **범주형**만
	* **카이제곱** 통계량

## 앙상블(Ensemble) 분석

* 주어진 자료로부터 여러 개의 예측모형을 만든 후 조합하여 하나의 최종 예측 모형을 만드는 방법
* 다중 모델 조합, 분류기 조합
* 이상값에 대응력이 높아짐
* 전체적인 예측값의 분산을 감소시켜 정확도를 높임
* 모형의 투명성이 떨어져 원인 분석에 적합하지 않음(ex 랜덤포레스트를 생각)
* 각 모형의 상호 연관성이 낮아여 정확도 향상

***부트스트랩** 은 주어진 자료에서 동일한 크기의 표본을 단순랜덤 복원추출로 뽑은 자료(한번도 선택되지 않은 원데이터 있을 수 있음,38.6%)*

**배깅**
* 주어진 자료에서 여러 개의 부트스트랩 자료를 생성 -> 각 부트스트랩 자료에 예측모형을 만듦 -> 결합 -> 최종 모형 만드는 방법
* **보팅**은 여러 개의 모형으로부터 산출된 결과를 다수결에 의해 최종 결과를 선정하는 과정
* 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정 나무들을 활용
* 훈련자료의 모잡단의 분포를 모르기 때문에 실전 문제에서는 평균예측모형을 구할 수 없다
	* 이를 해결하기 위해 배깅에서는 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상

**부스팅**
* 예측력이 약한 모형들을 결합 -> 강한 예측모형 만들기
* **Adaboost**
	* 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합 -> 최종분류기
	* 단, 가중치의 합은 1
* 훈련오차를 빠르고 쉽게 줄일 수 있음
* 배깅에 비해 예측오차가 향상되어 성능이 뛰어난 경우가 많음

**랜덤 포레스트**
* 배깅, 부스팅보다 더 많은 무작위성을 줌
* 약한 학습기들을 생성한 후 이를 선형결합 -> 최종 학습기 만듦
* 정확도가 좋음
* 설명하기 어려움
* 입력변수가 많으면, 배깅과 부스팅과 비슷하거나 좋은 예측력
* 분류분석에 사용하는 모델 + 과대적합/과소적합의 문제도 해결
* **보험갱신 여부를 고객의 인구통계학적 특성, 보함가입 채널, 상품 종류 등의 정보로 예측할 때 사용**

## 인공신경망 분석
* 신경망은 가중치를 반복적으로 조정하며 학습
* 입력 링크에서 여러 신호를 받아 새로운 활성화 수준 계산 -> 출력 링크로 출력 신호를 보냄

**계산**
* 전이함수(활성화 함수) : 출력을 결정하며 입력 신호의 가중치 합을 계산해 임계값과 비교, 가중치 합이 임계값보다 작으면 뉴력의 출력은 -1, 같거나 크면 +1 출력
	* 시그모이드 함수
		* 0~1사이 확률값
	* softmax 함수
		* 표준화지수 함수, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수
	* Relu 함수
		* 0이하 = 0, 0초과 = x

**입력변수**
모형이 복합하여 입력 자료의 선택에 매우 민감하다.
아래의 조건이 신경망 모형에 적합하다.
* 범주형 변수 : 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주의 빈도가 일정할 때
	* 일정빈도가 아니라면 => 각 범주의 빈도가 비슷하도록 설정
* 연속형 변수 : 입력변수 값들의 범위가 변수간의 큰 차이가 없을 때
	* 분포가 평균을 중심으로 대칭이 아니라면 => 로그변환

**학습모드**
1. 온라인 학습모드
	* 각 관측값을 순차적, 하나씩 신경망투입해서 학습
	* 속도가 빠름
	* 훈련자료가 비정상성과 같이 특이한 성질을 가진 경우 좋음
	* 국소최솟값에서 벗어나기 쉽다
2. 확률적 학습 모드(probabilistic)
	* 온라인 학습 모드와 같으나 신경망에 투입되는 관측값의 순서가 랜덤
3. 배치 학습 모드
	* 전체 훈련자료를 동시에 신경망에 투입

** 은닉층과 노드의 수**
* 수가 많으면 : 가중치가 많아져 과대적합
* 적으면 : 과소적합
* 은닉층 1개 : 범용 근사자이므로 매끄러운 함수를 근사적으로 표현
* 은닉노드 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 사용

**과대 적합 문제**
* 빈번하게 발생
* 해결 : 알고리즘 조기종료(검증오차가 증가하기 시작하면 중지), 가중치 감소 기법(학습할 수록 변경되는 수치를 줄임)

**범주 불균형 문제**
* 비용이 큰 분류 분석의 대상에 관측치가 현저히 부족해서 모형이 제대로 학습되지 않음

**포화 문제**
* 각 노드를 연결하는 가중치의 절대값이 커져 조정이 더 이상 이루어지지 않아 과소적합이 발생함


## 군집분석
* 결과는 군집분석 방법에 따라 다를 수 있다
* 군집의 개수나 구조에 대한 가정없이 데이터들 사이의 거리를 기준으로 군집화를 유도
* 마케팅 조사에서 소비자들의 상품구매활동이나 life style에 따른 소비자군을 분류하여 시장 전략 수립등에 활용

*요인분석은 유사한 변수를 함께 묶어주는 것이 목적*
*판별분석은 사전에 집단이 나뉘어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적*

**거리**
관측 데이터 간 유사성이나 근접성으로 -> 군집 판단
**연속형 변수 거리**
* 유클리디안 거리 : 가장 많이 사용, 변수들의 산포 정도 감안X
	* sqrt( (x-y)^2의 합 )
* 표준화 거리 : 표준편차로 척도 변환 후 유클리디안 거리 계산, 표준화하게 되면 왜곡을 피할 수 있음
* 마할라노비스 거리 : 통계적 개념 포함, 변수들의 산포 고려하여 표준화한 거리. 두 벡터 사이의 거리를 표준공분산으로 나눔.
	* sqrt( (x-y)S^(-1)(x-y) ).  , S : 공분산행려ㄹ
* 체비세프 거리 : max|xi-yi|
* 맨하탄 거리 : 블록으로 세는 거리
* 캔버라 거리 : 합 ( |xi-yi| / (xi+yi)
* 민코우스키 거리 : 맨하탄 + 유클리디안

**범주형 변수 거리**
* 자카드 거리  : (합집합 - 교집합) / 합집합
* 자카드 계수 : 교집합 / 합집합
* 코사인 거리 : 1 - A*B / ||A|| * ||B|| (*는 행렬 곱)
* 코사인 유사도 : A*B / ||A|| * ||B||

### 계층적 군집분석
* n개의 군집으로 시작해 점차 군집 개수를 줄이는 방법
* 합병형 방법, 분리형 방법
* 최단연결법 : 거리 계산 시 최단거리 계산
* 최장연결법 : 최장거리로 계싼
* 평균연결법 : 평균을 거리로 계산
* 와드연결법 : 군집내 편차들의 제곱합, 군집 간 정보의 손실을 최소화
* 군집화 : 덴드로그램으로 군집선정

### 비계층적 군집분석
* k-평균 군집분석
	* 주어진 데이터를 k개의 클러스터로 묶음
	* 각 클러스터와 거리 차이의 분산을 최소화
	* 원하는 군집 개수와 seed 정하기 -> seed를 중심으로 군집 형성 -> 각 데이터를 거리가 가까운 seed가 있는 군집으로 분류 -> seed값 다시 계산(모든 개체가 군집으로 할당될 때까지 반복)
	* 거리 계산을 통해 군집화 -> **연속형 변수**에 활용 가능
	* 초기 중심값의 선정에 따라 결과가 달라짐
	* 탐욕적 알고리즘이므로 최적이라는 보장은 없음
	* 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있음
	* 다양한 형태의 데이터에 적용가능

**실루엣 계수**
* 군집 모형 평가 기준
* 군집의 밀집정도를 계산, 군집 내의 거리와 군집 간의 거리를 기준으로 군집 분할의 성과 평가

## 연관분석
* 연관분석 = 장바구니분석 = 서열분석(a를 산다음에 b를 산다)

**측도**
1. 지지도(support)
	* 전체 거래 중 항목 A,B를 동시에 포함
	* 합집합 / 전체
2. 신뢰도(confidence)
	* A를 포함한 거래 중 A,B를 같이 포함할 확률
	* A를 산사람 중 B를 살 확률
	* 합집합 / P(A) == 지지도/P(A)
3. 향상도(Lift)
	* A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 B의 구매확률의 증가 비, A -> B는 A와 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다.
	* A를 샀을 때 B를 살 확률이 얼마나 증가하는가
	* P(B|A) / P(B) = P(A 합 B) / P(A)P(B) = A와 B가 동시에 포함된 거래 수/ (A를 포함하는 거래수 * B를 포함하는 거래 수) = 신뢰도/P(B)


###
* 순차분석 : 시간개념을 포함시켜 순차적인 구매 가능성이 큰 상품군을 찾아내는 기법

## SOM
* 비지도학습 군집분석
* 역전파 알고리즘 사용안함
* 위치 관계를 보존
* 승자 독점의 학습 규칙에 따라 입력 패턴과 가장 유사한 경쟁층 뉴런이 승
 
####
* 과대적합이 발생할 것으로 예상되면 학습을 종료하고 업데이트하는 과정을 반복해 방지가능
* 과대적합으로 생성된 모델은 분석 데이터에 최적화되었기 때문에 훈련 데이터의 작은 변화에 민감
* CART : 의사결정나무 중 연속형 변수를 예측
* 다차원척도 : 여러 대상 간의 거리가 주어져 있을 때, 대상들을 동일한 상대적 거리를 가진 실수 공간의 점들로 배치시키는 방법
* 와드연결법 : 계층적군집을 수행할 때 두 군집간의 거리를 군집내 오차제곱합에 기초하여 수행
* 검정력이란 대립가설이 맞을 때 그것을 받아들이는 확률
* 정보 : 데이터의 가공 및 상관관계 간 이해를 통해 패턴을 인식하고 의미를 부여한 데이터, 지식을 도출할 때 사용하는 데이터
* 유형분석 : 은행에서 대출심사를 할 때 소득, 카드사용액, 나이 등 해당 고객의 개인정보를 바탕으로 그 고객이 대출 상환을 잘하는 집단에 속할지 그렇지 않은 집단에 속할지 예측
* 내용기반 필터링 : 아이템에 대한 설명과 사용자 선호를 기반으로 하여 과거에 사용자가 좋아했던 것과 비슷한 아이템을 추천해주는 알고리즘
* 시계열분석
	* 지수평활법 : 일정기간의 평균을 이용하는 이동평균법과 달리 모든 시계열 자료를 사용하여 평균을 구하며, 시간의 흐름에 따라 최근 시계열에 더 많은 가중치를 부여하여 미래를 예측
	* AR모형 : 자기상관함수가 빠르게 감소하고 부분자기함수는 어느 시점에 절단점을 가진다
	* 정상시계열은 어떤 시점에서 평균과 분산 그리고 특정한 시차의 길이를 갖는 자기공분산을 측정하더라도 동일한 값을 갖는다
	* ARMA모형 : 약한 정상성을 가진 확률적 시계열을 표현하는데 사용
* 쌍체 t 검정 : 실험 이전의 집단과 실험 이후의 집단이 동일한 집단인 경우 사용하는 검정, 한 개인이 서로 다른 두 조건에서 짝을 지어 한 쌍이 연구대상이 되는 경우의 분석 방법
* ANOVA ? :  
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjEzMDM2MjY4MCwtMTkzNDU2MDA1LDEzND
c2NDkwMjMsMTUyOTE5Nzk1OSwtMTY3MzYxNDI2LC00NDUyNzMz
OTgsNjQ5Mjk4NTUxLDE3OTEwMTg2Nyw2NzQxNTM2MDAsLTIwNj
A5MTAyNSwtMTAxNDA5MjYwNiwxMTI1OTczOTg0LC0xMzgyMjI2
NCwxMTQ0MTI0NTQ3LDUxODc3OTA1OSwzNjAzNzcyMzcsOTc5MT
gwNjEsMTU1MzA2NDUyMiw2NjU5MDA0NDQsMTIwMDY3MzAzMl19

-->